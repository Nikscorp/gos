# 4 Сети хранения данных – архитектура и основные сервисы.

**Сеть хранения данных** (СХД, англ. Storage Area Network, SAN) — представляет собой архитектурное решение для подключения внешних устройств хранения данных, таких как дисковые массивы, ленточные библиотеки, оптические приводы к серверам таким образом, чтобы операционная система распознала подключенные ресурсы как локальные.
Большинство сетей хранения данных использует протокол SCSI для связи между серверами и устройствами хранения данных на уровне шинной топологии. Так как протокол SCSI не предназначен для формирования сетевых пакетов, в сетях хранения данных используются низкоуровневые **протоколы**:

* Fibre Channel Protocol (FCP), транспорт SCSI через Fibre Channel. Наиболее часто используемый на данный момент протокол. Существует в вариантах 1 Gbit/s, 2 Gbit/s, 4 Gbit/s, 8 Gbit/s, 10 Gbit/s, 16 Gbit/s, 20 Gbit/s.
* iSCSI, транспорт SCSI через TCP/IP.
* iSER, транспорт iSCSI через InfiniBand / RDMA.
* SRP, транспорт SCSI через InfiniBand / RDMA
* FCoE, транспортировка FCP/SCSI поверх «чистого» Ethernet.
* FCIP и iFCP, инкапсуляция и передача FCP/SCSI в пакетах IP.
* HyperSCSI, транспорт SCSI через Ethernet.
* FICON транспорт через Fibre Channel (используется только мейнфреймами).
* ATA over Ethernet, транспорт ATA через Ethernet.



**Технологически SAN состоит из следующих компонентов:**

* Узлы, ноды (nodes)
  * Дисковые массивы (системы хранения данных) — хранилища (таргеты [targets])
  * Серверы — потребители дисковых ресурсов (инициаторы [initiators]).
* Сетевая инфраструктура
  * Коммутаторы (и маршрутизаторы в сложных и распределённых системах)
  * Кабели

Для SAN ключевыми параметрами являются не только производительность, но и надёжность. Ведь если у сервера БД пропадёт сеть на пару секунд (или даже минут) — ну неприятно будет, но пережить можно. А если на это же время отвалится жёсткий диск с базой или с ОС, эффект будет куда более серьёзным. Поэтому все компоненты SAN обычно дублируются — порты в устройствах хранения и серверах, коммутаторы, линки между коммутаторами и, ключевая особенность SAN, по сравнению с LAN — дублирование на уровне всей инфраструктуры сетевых устройств — фабрики.**Фабрика** (fabric — что вообще-то в переводе с английского ткань, т.к. термин символизирует переплетённую схему подключения сетевых и конечных устройств, но термин уже устоялся) — совокупность коммутаторов, соединённых между собой межкоммутаторными линками (ISL — InterSwitch Link). Высоконадёжные SAN обязательно включают две (а иногда и более) фабрики, поскольку фабрика сама по себе — единая точка отказа. Фабрики могут иметь идентичную (зеркальную) топологию или различаться. Например одна фабрика может состоять из четырёх коммутаторов, а другая — из одного, и к ней могут быть подключены только высококритичные узлы.

**Различают следующие виды топологий фабрики:**

* **Каскад** — коммутаторы соединяются последовательно. Если их больше двух, то ненадёжно и непроизводительно.
* **Кольцо** — замкнутый каскад. Надёжнее просто каскада, хотя при большом количестве участников (больше 4) производительность будет страдать. А единичный сбой ISL или одного из коммутаторов превращает схему в каскад со всеми вытекающими.
* **Сетка (mesh)**. Бывает **Full Mesh** — когда каждый коммутатор соединяется с каждым. Характерно высокой надежностью, производительностью и ценой. Количество портов, требуемое под межкоммутаторные связи, с добавлением каждого нового коммутатора в схему растет экспоненциально. При определённой конфигурации просто не останется портов под узлы — все будут заняты под ISL. **Partial Mesh** — любое хаотическое объединение коммутаторов.
* **Центр/периферия** (Core/Edge) — близкая к классической топологии LAN, но без уровня распределения. Нередко хранилища подключаются к Core-коммутаторам, а серверы — к Edge. Хотя для хранилищ может быть выделен дополнительный слой (tier) Edge-коммутаторов. Также и хранилища и серверы могут быть подключены в один коммутатор для повышения производительности и снижения времени отклика (это называется локализацией). Такая топология характеризуется хорошей масштабируемостью и управляемостью.



**Архитектура дисковой подсистемы (ДПС)**
Все порты ДПС подключаются к контроллеру. Контроллер управляет дисками и кешем. Часть дисков используется для резервирования, поэтому объем памяти, предоставляемый пользователям обычно меньше реального объема. В ДПС может использоваться дублирование каналов связи, для повышения надежности. При выходе из строя одной линии связи, можно переключиться на вторую для доступа к диску. При нормальной работе каналы, по которым происходит доступ к дискам, могут быть фиксированы (запросы к одному диску всегда идут по одному каналу) или выбираться динамически (решение, по какому каналу отправлять запрос принимает контроллер в каждом отдельном случае). См. рисунок ниже.

![img](https://lh3.googleusercontent.com/QqsTzcm0nsGdyTmDQGB-vdTIlHFejR4IdyIIB_S2DV6nXTyUjLJPLPadAMZQHSTqNNpAiWjK8ua0do4YK5M4AFU3YwTv3n9TVfE64rb7K4n-3VJqUTlcV6Dn49MYlxA_fFYN1IHu)

**Для резервирования данных используются:**

* Горячий дисковый резерв - все диски дублируются, используется активное резервирование
* RAID массивы - избыточный массив независимых дисков.

**Для обеспечения устойчивости работоспособности ДПС используются разные техники:**

* Данные распределяют по нескольким дискам с помощью механизмов RAID и снабжают избыточными данными (блоки четности). 
* На каждом физическом диске данные закодированы кодом Хемминга. Кроме этого диск оснащен подсистемой самодиагностики, которая контролирует частоту ошибок, вибрацию шпинделя и т.д. Это позволяет проактивно прогнозировать отказы диска.
* Каждый диск подсоединен к контроллеру хотя бы через две внутренние шины.
* Контроллер дисковой подсистемы может быть продублирован. Выход одного экземпляра, автоматически будет активизировать следующий экземпляр. Схема Active-Standby.
* Используют периодическое мгновенное копирование для защиты от логических ошибок. Например, создание мгновенной копии данных через каждый час. Тогда в случае сбоя и уничтожения какой-то таблицы, она может быть восстановлена.
* Удаленное зеркалирование (копирование всех данных в удаленную сеть хранения данных) используют от физического уничтожения или повреждения оборудования (катастрофоустойчивость). В сочетании с мгновенным копированием эти сервисы гарантируют сохранение и консистентность данных даже для нескольких виртуальных дисков или дисковых подсистем.
* LUN (Logical Unit Number) маскирование защищает от несанкционированного доступа, упрощает работу системного администратора, защищает от случайных сбоев в работе приложений серверов и их оборудования.

**LUN (Logical Unit Number) маскирование** 

LUN не означает отдельный жесткий диск, скорее он определяет виртуальный раздел в RAID-массиве. При этом один и тот же виртуальный раздел массива может иметь разные значения LUN для разных хостов, которым этот LUN назначен. Также возможно наличие на одном хосте одинаковых LUN, принадлежащих разным системам хранения (разным SCSI Target ID).

Таким образом, полный адрес диска (физического раздела жёсткого диска) на SCSI-устройстве складывается из SCSI Target ID (уникального для хоста и определяемого драйвером) и LUN, уникального в пределах SCSI-устройства и назначаемого ему в настройках или автоматически по порядку.

**Преимущества**
Совместное использование систем хранения, как правило, упрощает администрирование и добавляет изрядную гибкость, поскольку кабели и дисковые массивы не нужно физически транспортировать и перекоммутировать от одного сервера к другому.

Другим преимуществом является возможность загружать сервера прямо из сети хранения. При такой конфигурации можно быстро и легко заменить сбойный сервер, переконфигурировав SAN таким образом, что сервер-замена будет загружаться с LUN'а (LUN, Logical Unit Number - адрес дискового устройства в SAN) сбойного сервера. Эта процедура может занять, например, полчаса. Идея относительно новая, но уже используется в новейших датацентрах.

Дополнительным преимуществом является возможность на хосте собрать RAID-зеркало из LUNов, которые презентованы хосту с двух разных дисковых массивов. В таком случае полный отказ одного из массивов не навредит хосту.

Также сети хранения помогают более эффективно восстанавливать работоспособность после сбоя. В SAN может входить удаленный участок со вторичным устройством хранения. В таком случае можно использовать репликацию — реализованную на уровне контроллеров массивов, либо при помощи специальных аппаратных устройств.