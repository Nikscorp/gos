## 16. Задача кластеризации как фундаментальная задача интеллектуального анализа данных, сопоставление с операцией группирования и задачей классификации. Различные постановки: разбиение, стохастическая, нечёткая, иерархическая, упорядочивание, однокластерная (последовательная). Примеры методов кластеризации для разных постановок.

Предварительно отмечу, что этой темы не было у нас в курсе вообще!!!

#### Задача кластеризации как фундаментальная задача интеллектуального анализа данных, сопоставление с операцией группирования и задачей классификации

http://www.machinelearning.ru/wiki/index.php?title=%D0%9A%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F

https://habr.com/ru/post/101338/

**Кластерный анализ** (Data clustering) — задача разбиения заданной [выборки](http://www.machinelearning.ru/wiki/index.php?title=Выборка) *объектов* (ситуаций) на непересекающиеся подмножества, называемые [кластерами](http://www.machinelearning.ru/wiki/index.php?title=Кластер&action=edit), так, чтобы каждый кластер состоял из схожих объектов, а объекты разных кластеров существенно отличались.

![](img\clustering.jpg)

Задача кластеризации относится к широкому классу задач [обучения без учителя](http://www.machinelearning.ru/wiki/index.php?title=Обучение_без_учителя).

Пусть ![X](http://www.machinelearning.ru/mimetex/?X) — множество объектов, ![Y](http://www.machinelearning.ru/mimetex/?Y) — множество номеров (имён, меток) кластеров. Задана функция расстояния между объектами ![\rho(x,x')](http://www.machinelearning.ru/mimetex/?\rho(x,x')). Имеется конечная обучающая выборка объектов ![X^m = \{ x_1, \dots, x_m \} \subset X](http://www.machinelearning.ru/mimetex/?X^m = \{ x_1, \dots, x_m \} \subset X). Требуется разбить выборку на непересекающиеся подмножества, называемые *кластерами*, так, чтобы каждый кластер состоял из объектов, близких по метрике ![\rho](http://www.machinelearning.ru/mimetex/?\rho), а объекты разных кластеров существенно отличались. При этом каждому объекту ![x_i\in X^m](http://www.machinelearning.ru/mimetex/?x_i\in X^m) приписывается номер кластера ![y_i](http://www.machinelearning.ru/mimetex/?y_i).

Примеры функций расстояния:

+ *Евклидово расстояние*
   Наиболее распространенная функция расстояния. Представляет собой геометрическим расстоянием в многомерном пространстве:
   ![img](https://habrastorage.org/getpro/habr/post_images/aa4/6e7/d7b/aa46e7d7b544dbaa221a43bb671fb43c.jpg)

+ *Квадрат евклидова расстояния*
   Применяется для придания большего веса более отдаленным друг от друга объектам. Это расстояние вычисляется следующим образом:
   ![img](https://habrastorage.org/getpro/habr/post_images/5dc/84a/1d6/5dc84a1d66392e9f11ba13be381ad036.jpg)

+ *Расстояние городских кварталов (манхэттенское расстояние)*
   Это расстояние является средним разностей по координатам. В большинстве  случаев эта мера расстояния приводит к таким же результатам, как и для  обычного расстояния Евклида. Однако для этой меры влияние отдельных  больших разностей (выбросов) уменьшается (т.к. они не возводятся в  квадрат). Формула для расчета манхэттенского расстояния:
   ![img](https://habrastorage.org/getpro/habr/post_images/930/f68/0a2/930f680a27caed2bed04f6b09a70a785.jpg)

*Алгоритм кластеризации* — это функция ![a:\, X\to Y](http://www.machinelearning.ru/mimetex/?a:\, X\to Y), которая любому объекту ![x\in X](http://www.machinelearning.ru/mimetex/?x\in X) ставит в соответствие номер кластера ![y\in Y](http://www.machinelearning.ru/mimetex/?y\in Y). Множество ![Y](http://www.machinelearning.ru/mimetex/?Y) в некоторых случаях известно заранее, однако чаще ставится задача  определить оптимальное число кластеров, с точки зрения того или иного *критерия качества* кластеризации.

![](img\classificclustering.jpg)

Кластеризация ([обучение без учителя](http://www.machinelearning.ru/wiki/index.php?title=Обучение_без_учителя)) отличается от [классификации](http://www.machinelearning.ru/wiki/index.php?title=Классификация) ([обучения с учителем](http://www.machinelearning.ru/wiki/index.php?title=Обучение_с_учителем)) тем, что метки исходных объектов ![y_i](http://www.machinelearning.ru/mimetex/?y_i) изначально не заданы, и даже может быть неизвестно само множество ![Y](http://www.machinelearning.ru/mimetex/?Y).

Решение задачи кластеризации принципиально неоднозначно, и тому есть несколько причин:

-  Не существует однозначно наилучшего критерия качества  кластеризации. Известен целый ряд эвристических критериев, а также ряд  алгоритмов, не имеющих чётко выраженного критерия, но осуществляющих  достаточно разумную кластеризацию «по построению». Все они могут давать  разные результаты.
-  Число кластеров, как правило, неизвестно заранее и устанавливается в соответствии с некоторым субъективным критерием.
-  Результат кластеризации существенно зависит от метрики, выбор которой, как правило, также субъективен и определяется экспертом.

######   Типы входных данных 

-  [Признаковое описание](http://www.machinelearning.ru/wiki/index.php?title=Признаковое_описание) объектов. Каждый объект описывается набором своих характеристик, называемых *признаками*. Признаки могут быть числовыми, порядковыми и категориальными.
-  [Матрица расстояний](http://www.machinelearning.ru/wiki/index.php?title=Матрица_расстояний&action=edit) между объектами. Каждый объект описывается расстояниями до всех остальных объектов обучающей выборки. 

######   Цели кластеризации 

-  [Понимание данных](http://www.machinelearning.ru/wiki/index.php?title=Понимание_данных&action=edit) путём выявления кластерной структуры. Разбиение выборки на группы  схожих объектов позволяет упростить дальнейшую обработку данных и  принятия решений, применяя к каждому кластеру свой метод анализа  (стратегия «[разделяй и властвуй](http://www.machinelearning.ru/wiki/index.php?title=Разделяй_и_властвуй&action=edit)»).
-  [Сжатие данных](http://www.machinelearning.ru/wiki/index.php?title=Сжатие_данных&action=edit). Если исходная выборка избыточно большая, то можно сократить её, оставив по одному наиболее типичному представителю от каждого кластера.
-  [Обнаружение новизны](http://www.machinelearning.ru/wiki/index.php?title=Обнаружение_новизны&action=edit) (novelty detection). Выделяются нетипичные объекты, которые не удаётся присоединить ни к одному из кластеров.

#### Различные постановки: разбиение, стохастическая, нечёткая, иерархическая, упорядочивание, однокластерная (последовательная). Примеры методов кластеризации для разных постановок.

+ Разбиение -- непосредственно разбиение объектов на конкретные кластеры, алгоритмы: k-means, алгоритмы иерархической кластеризации. (наверное)
+ Стохастическая -- хз что это, не могу нигде найти. Предположу, что выдвигается гипотеза о том, что каждый кластер представляет из себя некоторое распределение, а значит, все разделяемое пространство объектов -- смесь таких распределений. Значит, задача сведется к разделению смеси распределений. Алгоритм: Gaussian Mixture Models.

+ Нечеткая кластеризация -- Нечёткую кластеризацию от просто кластеризации отличает то, что объекты, которые подвергаются кластеризации, относятся к конкретному кластеру с  некой принадлежностью, а не однозначно, как это бывает при обычной  кластеризации. Например, при нечёткой кластеризации объект A относится к кластеру K1 с вероятностью 0.9, к кластеру K2 — с вероятностью   0.04 и к кластеру К3 — с вероятностью  0.06. При обычной же (чёткой)  кластеризации объект А был бы отнесён к кластеру K1. Алгоритм: c-means

+ Иерархические алгоритмы (также называемые алгоритмами таксономии) строят не одно разбиение выборки на непересекающиеся кластеры, а систему  вложенных разбиений. Т.о. на выходе мы получаем дерево кластеров, корнем которого является вся выборка, а листьями — наиболее мелкие кластера. Нисходящие алгоритмы работают по принципу «сверху-вниз»: в начале все объекты помещаются в один кластер, который затем разбивается на все более мелкие кластеры. Более распространены восходящие алгоритмы, которые в начале работы помещают каждый объект в отдельный кластер, а затем объединяют кластеры во все более крупные, пока все объекты выборки не будут содержаться в одном кластере. Таким образом строится система вложенных разбиений.

+ Упорядочивание -- не знаю, вероятно, выдача меток объектам, при этом кластеры не образуют никаких вложенных структур. Алгоритмы: K-Means

+ Однокластерная -- (выдуманное определение) в данной постановке размечаются объекты, принадлежащие к одному и тому же кластеру, остальные считаются не принадлежащими к нему, каждый кластер ищется последовательно, одним из примеров алгоритмов, который использует данные принципы, алгоритм плотностной кластеризации DBScan 

![](img\algos.png)

На картинке демонстрация результатов работы различных алгоритмов кластеризации. Как видно, для разных данных разные алгоритмы по-разному работают.

